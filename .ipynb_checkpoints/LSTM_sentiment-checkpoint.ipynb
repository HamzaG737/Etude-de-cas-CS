{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=pd.read_csv('/home/hamza/projects/filiere/etude_de_cas/data/trainset_sent.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capitalisme tueur\n",
      "++++++++++++++++++++\n",
      "Le secteur agricole africain : les d√©fis √† relever |\n",
      "++++++++++++++++++++\n",
      "Revue de presse, suite ... Article de la Gazette du Mantois N¬∞110, o√π il est question de traitement des d√©chets ... .\n",
      "++++++++++++++++++++\n",
      "Bourse de 23 000 $ (=droits de scolarit√©-4 ans) - Bac en d√©veloppement durable et zone c√¥ti√®re. developpementdurable\n",
      "++++++++++++++++++++\n",
      "alors ce d√©bat sur les √©oliennes apr√®s la soir√©e. ;)\n",
      "++++++++++++++++++++\n",
      "biodiversit√© Les vacanciers invit√©s √† nettoyer la nature et devenir des √©co-h√©ros\n",
      "++++++++++++++++++++\n",
      "BTP Durable DD Ecologie Exemplarit√©s prim√©es\n",
      "++++++++++++++++++++\n",
      "La Grande Bretagne veut alimenter tous les foyers en √©nergies renouvelables avec 7000 √©oliennes\n",
      "++++++++++++++++++++\n",
      "Les banques s‚Äôinvitent dans l‚Äô√©cosyst√®me du ¬´‚Äâcrowdfunding‚Äâ¬ª, Banque - Assurances\n",
      "++++++++++++++++++++\n",
      "‚Äú: Les OGM et la panne de la biodiversit√© agricolevia ‚Äù\n",
      "++++++++++++++++++++\n",
      "Suppression de Ecotaxe : le mot √©cologie de SegoleneRoyal rime avec d√©magogie La menace des routiers va co√ªter plusieurs milliards ‚Ç¨ merci\n",
      "++++++++++++++++++++\n",
      "Vulcano‚Ä¶ vulcano √Æles √©oliennes sicile volcano volcan souffre √©ruption Aeolian Islands aventure et volcans\n",
      "++++++++++++++++++++\n",
      "La Presse+: Un groupe √©cologiste s'inqui√®te du sort des parcs nationaux\n",
      "++++++++++++++++++++\n",
      "A 8h35, je re√ßois , la ministre de l'Ecologie suretbourdindirect\n",
      "++++++++++++++++++++\n",
      "DirectAN L'UMP \"vouloir √©chapper √† l'impot est un d√©lit pour les √©cologistes\" hahahaha\n",
      "++++++++++++++++++++\n",
      "t'es la meilleure Laura üíã tu es tellement dr√¥le et sympas et surtout naturelle! Tu d√©gage une telle √©nergie positive! Je t'aime‚ù§\n",
      "++++++++++++++++++++\n",
      "Le vrai impact √©cologique du Tour de France ‚ñ∫‚ñ∫TDF\n",
      "++++++++++++++++++++\n",
      "S√©natoriales: V√©ronique B√©r√©govoy candidate √©cologiste en Seine-Maritime 76 eelv rouen dieppe lehavre elbeuf\n",
      "++++++++++++++++++++\n",
      "Le gouvernement wallon renonce aux objectifs de production d'√©nergie renouvelable, \"trop ambitieux et dogmatiques\"\n",
      "++++++++++++++++++++\n",
      "_h voil√†. √âcologiste ti√®de.\n",
      "++++++++++++++++++++\n",
      "chatellerault Dans l'ex-usine New Fabris, VMH √ânergie a lanc√© la production de panneaux solairesnouvellerepublique\n",
      "++++++++++++++++++++\n",
      "[VotreEnergie] La r√©novation √©nerg√©tique √† l‚Äôhonneur pour la f√™te de l‚Äô√©nergie !\n",
      "++++++++++++++++++++\n",
      "Il y a une transition a faire vers les √©nergies vertes. √áa se fera lentement. Peut √™tre d'ici 2050, mais pas avant.\n",
      "++++++++++++++++++++\n",
      "Scandale des √©oliennes : corruption des √©lus et c‚Ä¶¬†:\n",
      "++++++++++++++++++++\n",
      "Des puits abandonn√©s inqui√®tent des¬†√©cologistes\n",
      "++++++++++++++++++++\n",
      "ecologie Aux Etats-Unis, ils polluent pour le plaisir - et pour emmerder les √©colos\n",
      "++++++++++++++++++++\n",
      "trkl avec les √©oliennes\n",
      "++++++++++++++++++++\n",
      "Valls esp√®re que les morts du r√©chauffement climatique seront frondeurs, pour MLP c'est plut√¥t noirs & beurs actu\n",
      "++++++++++++++++++++\n",
      "DD RSE Exaptation: rendre op√©rationnel le d√©veloppement durable dans votre entreprise\n",
      "++++++++++++++++++++\n",
      "Des arbres sur le toit de maisons √©cologiques au Vietnam.\n",
      "++++++++++++++++++++\n",
      "ecologie Etang-Sal√©: Le d√©veloppement durable invit√© au march√© des cr√©ateurs\n",
      "++++++++++++++++++++\n",
      "Eolien en mer: baisse du nombre de nouvelles √©oliennes ‚Äì Entreprises ‚Äì Actualit√© ‚Äì Trends.be\n",
      "++++++++++++++++++++\n",
      "Groen et Ecolo forment un groupe commun au S√©nat: Les partis √©cologistes francophone et n√©erlandopho ... Belgique\n",
      "++++++++++++++++++++\n",
      "L'observatoire de la biodiversit√© du Nord - Pas de Calais: Conservatoire Botanique National... nouveautesducrid\n",
      "++++++++++++++++++++\n",
      "Zoo de Romagne : naissance en captivit√© d'un b√©b√© bonobo: C'est une naissance rarissime : une petite femelle b ...\n",
      "++++++++++++++++++++\n",
      "Environnement ‚ñ∫ M√©diterran√©e : bient√¥t des √©oliennes flottantes en mer !: D‚Äôici 2015, deux pr ... ‚óÑDevDurable\n",
      "++++++++++++++++++++\n",
      "RT_Baupin: Toute incitation √† la haine est √©trang√®re aux valeurs des √©cologistes\n",
      "++++++++++++++++++++\n",
      "\"int√©grer le d√©veloppement durable dans les phases de conception et fabrication\" EcoVerte FondamCNRS\n",
      "++++++++++++++++++++\n",
      "Royal: le pr√©sident n'aime pas les pauvres? \"c'est n'importe quoi\"\n",
      "++++++++++++++++++++\n",
      "Une web s√©rie pour sensibiliser aux gestes √©cologiques en entreprise\n",
      "++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "l=random.choices(list(range(len(trainset))),k=40)\n",
    "for sentence in l :\n",
    "    print(trainset['text'][sentence])\n",
    "    print('++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x) :\n",
    "    return x.replace('  ','')\n",
    "\n",
    "trainset['text']= trainset['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_sentences = list(trainset.text.values)\n",
    "all_text = ' '.join(all_sentences)\n",
    "# create a list of words\n",
    "words = all_text.split()# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer les mots en indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "## transform dataset to indexes \n",
    "sentences_indexes = []\n",
    "for sentence in all_sentences:\n",
    "    r = [vocab_to_int[w] for w in sentence.split()]\n",
    "    sentences_indexes.append(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define padding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_indexes) :\n",
    "    X_lengths = [len(sentence) for sentence in sentences_indexes] # create an empty matrix with padding tokens\n",
    "    longest_sent = max(X_lengths)\n",
    "    padded_X = np.zeros((len(sentences_indexes), longest_sent))  # copy over the actual sequences\n",
    "\n",
    "    for i, x_len in enumerate(X_lengths):\n",
    "        sequence = sentences_indexes[i]\n",
    "        padded_X[i, 0:x_len] = sequence[:x_len]# padded_X looks like:\n",
    "    return padded_X,np.asarray(X_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7048, 28)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x , X_lengths =pad_sentences(sentences_indexes)\n",
    "labels=trainset.polarity.values\n",
    "\n",
    "## divide into train, validation \n",
    "\n",
    "split_frac = 0.2 # 80% train, 20% validation\n",
    "split_id = int(split_frac * len(padded_x))\n",
    "val_sentences, train_sentences = padded_x[:split_id], padded_x[split_id:]\n",
    "val_labels, train_labels = labels[:split_id], labels[split_id:]\n",
    "val_lengths,train_lengths = X_lengths[:split_id], X_lengths[split_id:]\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_lengths),torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_lengths),torch.from_numpy(val_labels) )\n",
    "#test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "#test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define LSTM architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The class is an implementation of a bi-lstm class. \n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_size,n_layers):\n",
    "        \n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_dim,n_layers, bidirectional=True)\n",
    "        self.fc  = torch.nn.Linear(2*hidden_dim,output_size)\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.n_layers * 2, batch_size, self.hidden_dim)\n",
    "        hidden_b = torch.randn(self.n_layers * 2 , batch_size, self.hidden_dim)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "        \n",
    "    def forward(self,padded_x,x_lengths):\n",
    "         \n",
    "        X = self.embedding(padded_x)\n",
    "        batch_size,length_sentence,_=X.size()\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embeddings = torch.nn.utils.rnn.pack_padded_sequence(X, x_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        outputs, (h_n,c_n) = self.lstm(embeddings,self.hidden)\n",
    "        \n",
    "        #Unpack outputs (remove paddings)\n",
    "        outputs, _= torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        h_n_per = h_n.permute(1,0,2)\n",
    "        \n",
    "        outputs_reshaped = h_n_per.reshape(h_n_per.shape[0],-1)\n",
    "        \n",
    "        #outputs.view(, outputs.shape[2])\n",
    "        \n",
    "        out=self.fc(outputs_reshaped)\n",
    "    \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training parameters\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 3\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 1\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim,hidden_dim,output_size, n_layers)\n",
    "\n",
    "lr=0.01\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):    \n",
    "    for inputs, x_lengths, labels in train_loader:\n",
    "        counter += 1\n",
    "        #h = tuple([e.data for e in h])\n",
    "        #inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(inputs.long(), x_lengths)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        if counter%print_every == 0:\n",
    "            #val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, x_length, labels in val_loader:\n",
    "                #val_h = tuple([each.data for each in val_h])\n",
    "                #inp, lab = inp.to(device), lab.to(device)\n",
    "                out = model(inp.long(), x_length)\n",
    "                val_loss = criterion(out, labels)\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
