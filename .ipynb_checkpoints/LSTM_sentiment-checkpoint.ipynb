{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=pd.read_csv('/home/hamza/projects/filiere/etude_de_cas/data/trainset_sent.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>pol_symbol</th>\n",
       "      <th>nature</th>\n",
       "      <th>emotion</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>487830913032519681</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Et pendant ce temps la elles tournent  via</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>505675738155347968</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>.    vous oubliez l  '  abbé Pierre responsabl...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>519392800002297857</td>\n",
       "      <td>+</td>\n",
       "      <td>OPINION</td>\n",
       "      <td>VALORISATION</td>\n",
       "      <td>[  LoiRoyal  ]   le solaire  :   l  '  énergie...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>506828044732878848</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>คันหู วาเลนไทน์ 2010  :    via  OUI , le récha...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>488949864600829953</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Correze - MAISON A OSSATURE BOIS ECOLOGIQUE  :...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7043</th>\n",
       "      <td>520252139172810753</td>\n",
       "      <td>+</td>\n",
       "      <td>OPINION</td>\n",
       "      <td>VALORISATION</td>\n",
       "      <td>Et une très bonne nouvelle pour les Français p...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7044</th>\n",
       "      <td>489438791703470080</td>\n",
       "      <td>-</td>\n",
       "      <td>SENTIMENT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Biodiversité et climat  :   Deux crises qui gl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7045</th>\n",
       "      <td>507083059359789056</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ecologie « La Constellation du chien », roman ...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7046</th>\n",
       "      <td>518726125501956096</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Manif pour tous   :   le sénateur écologiste J...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7047</th>\n",
       "      <td>487503555167649793</td>\n",
       "      <td>=</td>\n",
       "      <td>INFORMATION</td>\n",
       "      <td>NaN</td>\n",
       "      <td>SaintBrieuc  .   Soixante-deux  éoliennes  off...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7048 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id pol_symbol       nature       emotion  \\\n",
       "0     487830913032519681          =  INFORMATION           NaN   \n",
       "1     505675738155347968          =  INFORMATION           NaN   \n",
       "2     519392800002297857          +      OPINION  VALORISATION   \n",
       "3     506828044732878848          =  INFORMATION           NaN   \n",
       "4     488949864600829953          =  INFORMATION           NaN   \n",
       "...                  ...        ...          ...           ...   \n",
       "7043  520252139172810753          +      OPINION  VALORISATION   \n",
       "7044  489438791703470080          -    SENTIMENT           NaN   \n",
       "7045  507083059359789056          =  INFORMATION           NaN   \n",
       "7046  518726125501956096          =  INFORMATION           NaN   \n",
       "7047  487503555167649793          =  INFORMATION           NaN   \n",
       "\n",
       "                                                   text  polarity  \n",
       "0            Et pendant ce temps la elles tournent  via         2  \n",
       "1     .    vous oubliez l  '  abbé Pierre responsabl...         2  \n",
       "2     [  LoiRoyal  ]   le solaire  :   l  '  énergie...         1  \n",
       "3     คันหู วาเลนไทน์ 2010  :    via  OUI , le récha...         2  \n",
       "4     Correze - MAISON A OSSATURE BOIS ECOLOGIQUE  :...         2  \n",
       "...                                                 ...       ...  \n",
       "7043  Et une très bonne nouvelle pour les Français p...         1  \n",
       "7044  Biodiversité et climat  :   Deux crises qui gl...         0  \n",
       "7045  ecologie « La Constellation du chien », roman ...         2  \n",
       "7046  Manif pour tous   :   le sénateur écologiste J...         2  \n",
       "7047  SaintBrieuc  .   Soixante-deux  éoliennes  off...         2  \n",
       "\n",
       "[7048 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_sentences = list(trainset.text.values)\n",
    "all_text = ' '.join(all_sentences)\n",
    "# create a list of words\n",
    "words = all_text.split()# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer les mots en indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "## transform dataset to indexes \n",
    "sentences_indexes = []\n",
    "for sentence in all_sentences:\n",
    "    r = [vocab_to_int[w] for w in sentence.split()]\n",
    "    sentences_indexes.append(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define padding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_indexes) :\n",
    "    X_lengths = [len(sentence) for sentence in sentences_indexes] # create an empty matrix with padding tokens\n",
    "    longest_sent = max(X_lengths)\n",
    "    padded_X = np.zeros((len(sentences_indexes), longest_sent))  # copy over the actual sequences\n",
    "\n",
    "    for i, x_len in enumerate(X_lengths):\n",
    "        sequence = sentences_indexes[i]\n",
    "        padded_X[i, 0:x_len] = sequence[:x_len]# padded_X looks like:\n",
    "    return padded_X,np.asarray(X_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x , X_lengths =pad_sentences(sentences_indexes)\n",
    "labels=trainset.polarity.values\n",
    "\n",
    "## divide into train, validation \n",
    "\n",
    "split_frac = 0.2 # 80% train, 20% validation\n",
    "split_id = int(split_frac * len(padded_x))\n",
    "val_sentences, train_sentences = padded_x[:split_id], padded_x[split_id:]\n",
    "val_labels, train_labels = labels[:split_id], labels[split_id:]\n",
    "val_lengths,train_lengths = X_lengths[:split_id], X_lengths[split_id:]\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_lengths),torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_lengths),torch.from_numpy(val_labels) )\n",
    "#test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "#test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define LSTM architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The class is an implementation of a bi-lstm class. \n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_size,n_layers):\n",
    "        \n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_dim,n_layers, bidirectional=True)\n",
    "        self.fc  = torch.nn.Linear(2*hidden_dim,output_size)\n",
    "    \n",
    "    def init_hidden(self,batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.n_layers, batch_size, self.hidden_dim)\n",
    "        hidden_b = torch.randn(self.n_layers, batch_size, self.hidden_dim)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "        \n",
    "    def forward(self,padded_x,x_lengths):\n",
    "         \n",
    "        X = self.embedding(padded_x)\n",
    "        \n",
    "        \n",
    "        batch_size,length_sentence,_=padded_x.size()\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        \n",
    "        embeddings = torch.nn.utils.rnn.pack_padded_sequence(X, x_lengths, batch_first=True)\n",
    "        \n",
    "        outputs, self.hidden = self.lstm(embeddings,self.hidden)\n",
    "        \n",
    "        #Unpack outputs (remove paddings)\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        outputs = outputs.contiguous()\n",
    "        \n",
    "        outputs_reshaped = outputs.view(-1, outputs.shape[2])\n",
    "        \n",
    "        out=self.fc(outputs_reshaped)\n",
    "    \n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training parameters\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 3\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 1\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim,hidden_dim,output_size, n_layers)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 36])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-f9e9af3a2ee7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-41-7eebe222875a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, padded_x, x_lengths)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlength_sentence\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadded_x\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_hidden\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "epochs = 2\n",
    "counter = 0\n",
    "print_every = 1000\n",
    "clip = 5\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):    \n",
    "    for inputs, x_lengths, labels in train_loader:\n",
    "        counter += 1\n",
    "        #h = tuple([e.data for e in h])\n",
    "        #inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        print(inputs.size())\n",
    "        output = model(inputs.long(), x_lengths)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "        \"\"\"if counter%print_every == 0:\n",
    "            #val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            model.eval()\n",
    "            for inp, x_length, labels in val_loader:\n",
    "                #val_h = tuple([each.data for each in val_h])\n",
    "                #inp, lab = inp.to(device), lab.to(device)\n",
    "                out = model(inp, x_length)\n",
    "                val_loss = criterion(out.squeeze(), lab.float())\n",
    "                val_losses.append(val_loss.item())\n",
    "                \n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}...\".format(loss.item()),\n",
    "                  \"Val Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            if np.mean(val_losses) <= valid_loss_min:\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
