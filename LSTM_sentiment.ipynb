{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import librairies \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import re\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=pd.read_csv('/home/hamza/projects/filiere/etude_de_cas/data/trainset_sent.csv',lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un √©l√®ve de l  '  ISEN de la majeure √ânergie   :  \"on a de la chance √™tre en Bretagne, une r√©gion qui mise vraiment sur les √©nergies renouvelables  !  \"\n",
      "++++++++++++++++++++\n",
      ".      :    :    :   Plus de 200 villes renforcent leur R√âSILIENCE face au changement climatique\n",
      "++++++++++++++++++++\n",
      "La seule vedette vraiment √©cologique\n",
      "++++++++++++++++++++\n",
      "L‚Äô√©cologie est une responsabilit√©, pas une marchandise - Jean-Pierre Dupuy - Lib√©ration |\n",
      "++++++++++++++++++++\n",
      "Biodiversit√© du jardin urbain araign√©e  spider jardin garden LaGarenneColombes\n",
      "++++++++++++++++++++\n",
      "video La rentr√©e de l  '  ESIEE Ecole d‚Äôing√©nieurs en G√©nie Electrique, Informatique, T√©l√©communication, DD Picardie\n",
      "++++++++++++++++++++\n",
      "Les √©oliennes favorisent l  '  exploitation du charbon et du gaz   .    .    .    Jean Louis Butr√© explique bien cela   :   merci  .\n",
      "++++++++++++++++++++\n",
      "√âoliennes flottantes - Un site d  '  essais autoris√© en M√©diterran√©e\n",
      "++++++++++++++++++++\n",
      "Aujourd  '  hui lancement des √©quipements photovolta√Øques pour 7 centres de sant√© √† Diffa Niger  refugees\n",
      "++++++++++++++++++++\n",
      "La Commission lance une consultation publique sur la derni √®re strat √©gie macro-r √©gionale\n",
      "++++++++++++++++++++\n",
      "Apiculture   :    mobilise 40 millions pour le plan abeille  Miel Qui vient en sept  .   pour ma r√©colte   ?   Bonnieux\n",
      "++++++++++++++++++++\n",
      "\"Des auberges de jeunesse qui offrent une exp√©rience au milieu de la nature    .    .    .   \"\n",
      "++++++++++++++++++++\n",
      "Esp√©rons-le  ‚Äú  :   BOUM  :   Le solaire   :   premi√®re √©nergie mondiale en 2050\n",
      "++++++++++++++++++++\n",
      "le mit recycle les vieilles batteries pour cr√©er des panneaux solaires  Jeux\n",
      "++++++++++++++++++++\n",
      "G√©orisques   :   un portail web pour mieux conna√Ætre les risques sur le territoire\n",
      "++++++++++++++++++++\n",
      "_E1  est Charg√© projet \"Tourisme nature d√©veloppement durable\" Comit√© r√©gion IdF   .    .    .   il n  '  est pas membre f√©d√©ral EELV\n",
      "++++++++++++++++++++\n",
      "r√©chauffement climatique  :    dans les pays froids les chauds meurent, quel travail   !\n",
      "++++++++++++++++++++\n",
      "Du crowdfunding breton permet la sauvegarde de l  '  abeille noire d  '  Ouessant\n",
      "++++++++++++++++++++\n",
      "Faux arbres, vraies √©oliennes - S‚Äôil y a bien un concept terrien qui a le don d‚Äôagacer les Eyldar, c‚Äôest   .    .    .\n",
      "++++++++++++++++++++\n",
      "La s√©lection du jour  !   Parc du Pilat,de la cueillette bio √† l  '  √©nergie solaire  :    rhonealpestv\n",
      "++++++++++++++++++++\n",
      "Plus de 200 villes renforcent leur r√©silience face au changement climatique  :   Pour att√©nuer les impa   .    .    .     √©cologie\n",
      "++++++++++++++++++++\n",
      "La Lettre \"Mobilit√© Durable\" du Cerema est sortie  !    avec notamment   ou\n",
      "++++++++++++++++++++\n",
      "et tous les √©cologistes semblent charm√©s\n",
      "++++++++++++++++++++\n",
      "Pr√©cision du Groupe √©cologiste du S√©nat sur la r√©serve parlementaire  by\n",
      "++++++++++++++++++++\n",
      ".  _m \"L  '  abandon de l  '  Ecotaxe  :   un nouveau signal du manque d  '  int√©r√™t du gouvernement pour les questions d  '  √©cologie\"\n",
      "++++++++++++++++++++\n",
      "Fili√®res vertes et Insertion ‚Äì Sin√©o ‚Äì Nettoyage √©cologique de v√©hicules\n",
      "++++++++++++++++++++\n",
      "Et bonne journ√©e RT  Actuellement la neige est noire au Groenland  .   Et c‚Äôest inqui√©tant\n",
      "++++++++++++++++++++\n",
      "MDRRR IL SONT CONS SES PALESTINIENS  ?    ?   Le truc le moins possibles ils tirent sur leurs centrales electriques üòπüòπüòπ\n",
      "++++++++++++++++++++\n",
      "Rosneft et ExxonMobil menacent le berceau des ours polaires en Russie      via\n",
      "++++++++++++++++++++\n",
      "Dispositif relatif aux conditions sociales du pays d  '  accueil\n",
      "++++++++++++++++++++\n",
      "Y en a une a qui √ßa va faire bizarre le changement climatique entre le Texas et la France   !\n",
      "++++++++++++++++++++\n",
      "Les champs d  '  eoliennes raccord√©es √† rien et bien subventionn√©es sont pourtant bien la trace d  '  une pr√©sence ;-  )\n",
      "++++++++++++++++++++\n",
      "ClickClean   :   l  '  ONG Greenpeace pointe du doigt l‚Äôimpact du Web sur l  '   √©cologie\n",
      "++++++++++++++++++++\n",
      "Le lobbying, les √©cologistes s  '  en plaignent lorsque √ßa vient des autres notamment chasseurs et agriculteurs  .   Mais   .    .    .\n",
      "++++++++++++++++++++\n",
      "Le d√©cret sur l  '  Eco-PTZ et le cr√©dit d  '  imp√¥t d√©veloppement durable est paru immo defiscalisation via\n",
      "++++++++++++++++++++\n",
      "Balcombe r√©pond au gaz de schiste par l‚Äô√©nergie solaire\n",
      "++++++++++++++++++++\n",
      "Une chienne adopte un lionceau dans un zoo de Pologne\n",
      "++++++++++++++++++++\n",
      "Inauguration du tout nouveau Jardin communautaire et √©cologique Mont-Bleu  .    Enfin  !\n",
      "++++++++++++++++++++\n",
      "Interxion passe au 100   %   renouvelable -   datacenter\n",
      "++++++++++++++++++++\n",
      "ONVPSM On peut aussi tjs esp√©rer une remise en question de notre majorit√© vers la gauche et les √©cologistes  uniondelagauche‚Äù\n",
      "++++++++++++++++++++\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "l=random.choices(list(range(len(trainset))),k=40)\n",
    "for sentence in l :\n",
    "    print(trainset['text'][sentence])\n",
    "    print('++++++++++++++++++++')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(x) :\n",
    "    return x.replace('  ','')\n",
    "\n",
    "trainset['text']= trainset['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_sentences = list(trainset.text.values)\n",
    "all_text = ' '.join(all_sentences)\n",
    "# create a list of words\n",
    "words = all_text.split()# Count all the words using Counter Method\n",
    "count_words = Counter(words)\n",
    "\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer les mots en indices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_to_int = {w:i+1 for i, (w,c) in enumerate(sorted_words)}\n",
    "\n",
    "## transform dataset to indexes \n",
    "sentences_indexes = []\n",
    "for sentence in all_sentences:\n",
    "    r = [vocab_to_int[w] for w in sentence.split()]\n",
    "    sentences_indexes.append(r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define padding function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentences(sentences_indexes) :\n",
    "    X_lengths = [len(sentence) for sentence in sentences_indexes] # create an empty matrix with padding tokens\n",
    "    longest_sent = max(X_lengths)\n",
    "    padded_X = np.zeros((len(sentences_indexes), longest_sent))  # copy over the actual sequences\n",
    "\n",
    "    for i, x_len in enumerate(X_lengths):\n",
    "        sequence = sentences_indexes[i]\n",
    "        padded_X[i, 0:x_len] = sequence[:x_len]# padded_X looks like:\n",
    "    return padded_X,np.asarray(X_lengths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_x , X_lengths =pad_sentences(sentences_indexes)\n",
    "labels=trainset.polarity.values\n",
    "\n",
    "## divide into train, validation \n",
    "\n",
    "split_frac = 0.2 # 80% train, 20% validation\n",
    "split_id = int(split_frac * len(padded_x))\n",
    "val_sentences, train_sentences = padded_x[:split_id], padded_x[split_id:]\n",
    "val_labels, train_labels = labels[:split_id], labels[split_id:]\n",
    "val_lengths,train_lengths = X_lengths[:split_id], X_lengths[split_id:]\n",
    "\n",
    "train_data = TensorDataset(torch.from_numpy(train_sentences), torch.from_numpy(train_lengths),torch.from_numpy(train_labels))\n",
    "val_data = TensorDataset(torch.from_numpy(val_sentences), torch.from_numpy(val_lengths),torch.from_numpy(val_labels) )\n",
    "#test_data = TensorDataset(torch.from_numpy(test_sentences), torch.from_numpy(test_labels))\n",
    "\n",
    "batch_size = 256\n",
    "\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "val_loader = DataLoader(val_data, shuffle=True, batch_size=batch_size)\n",
    "#test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define LSTM architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    The class is an implementation of a bi-lstm class. \n",
    "    \"\"\"\n",
    "   \n",
    "    def __init__(self,vocab_size,embedding_dim,hidden_dim,output_size,n_layers):\n",
    "        \n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        self.embedding_dim  = embedding_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=embedding_dim, padding_idx=0)\n",
    "        self.lstm = torch.nn.LSTM(embedding_dim,hidden_dim,n_layers, bidirectional=True)\n",
    "        \n",
    "        self.fc1  = torch.nn.Linear(2*hidden_dim,512)\n",
    "        self.fc2  = torch.nn.Linear(512,output_size)\n",
    "        self.leaky=nn.LeakyReLU()\n",
    "    def init_hidden(self,batch_size):\n",
    "        # the weights are of the form (nb_layers, batch_size, nb_lstm_units)\n",
    "        hidden_a = torch.randn(self.n_layers * 2, batch_size, self.hidden_dim)\n",
    "        hidden_b = torch.randn(self.n_layers * 2 , batch_size, self.hidden_dim)\n",
    "\n",
    "        hidden_a = Variable(hidden_a)\n",
    "        hidden_b = Variable(hidden_b)\n",
    "\n",
    "        return (hidden_a, hidden_b)\n",
    "        \n",
    "    def forward(self,padded_x,x_lengths):\n",
    "         \n",
    "        X = self.embedding(padded_x)\n",
    "        batch_size,length_sentence,_=X.size()\n",
    "        \n",
    "        self.hidden = self.init_hidden(batch_size)\n",
    "        embeddings = torch.nn.utils.rnn.pack_padded_sequence(X, x_lengths, batch_first=True,enforce_sorted=False)\n",
    "        \n",
    "        outputs, (h_n,c_n) = self.lstm(embeddings,self.hidden)\n",
    "        \n",
    "        #Unpack outputs (remove paddings)\n",
    "        outputs, _= torch.nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "        \n",
    "        h_n_per = h_n.permute(1,0,2)\n",
    "        \n",
    "        outputs_reshaped = h_n_per.reshape(h_n_per.shape[0],-1)\n",
    "        \n",
    "        #outputs.view(, outputs.shape[2])\n",
    "        \n",
    "        out=self.leaky(self.fc1(outputs_reshaped))\n",
    "    \n",
    "        return self.fc2(out)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### training parameters\n",
    "vocab_size = len(vocab_to_int) + 1\n",
    "output_size = 3\n",
    "embedding_dim = 400\n",
    "hidden_dim = 512\n",
    "n_layers = 1\n",
    "\n",
    "model = LSTM(vocab_size, embedding_dim,hidden_dim,output_size, n_layers)\n",
    "\n",
    "lr=1e-3\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## define training function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/10... Loss: 1.018573... Val accuracy: 0.526615\n",
      "Epoch: 2/10... Loss: 0.867895... Val accuracy: 0.543648\n",
      "Epoch: 3/10... Loss: 0.342712... Val accuracy: 0.533712\n",
      "Epoch: 4/10... Loss: 0.146317... Val accuracy: 0.583392\n",
      "Epoch: 5/10... Loss: 0.070909... Val accuracy: 0.535131\n",
      "Epoch: 6/10... Loss: 0.013835... Val accuracy: 0.577005\n",
      "Epoch: 7/10... Loss: 0.002173... Val accuracy: 0.567069\n",
      "Epoch: 8/10... Loss: 0.003915... Val accuracy: 0.569908\n",
      "Epoch: 9/10... Loss: 0.002644... Val accuracy: 0.561391\n",
      "Epoch: 10/10... Loss: 0.000563... Val accuracy: 0.568488\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "counter = 0\n",
    "clip = 5\n",
    "valid_acc_max = -np.Inf\n",
    "\n",
    "model.train()\n",
    "for i in range(epochs):    \n",
    "    for inputs, x_lengths, labels in train_loader:\n",
    "        counter += 1\n",
    "        #h = tuple([e.data for e in h])\n",
    "        #inputs, labels = inputs.to(device), labels.to(device)\n",
    "        model.zero_grad()\n",
    "        output = model(inputs.long(), x_lengths)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "            \n",
    "    model.eval()\n",
    "    accuracy=0\n",
    "    is_equal=[]\n",
    "    for inp, x_length, labels in val_loader:\n",
    "        #val_h = tuple([each.data for each in val_h])\n",
    "        #inp, lab = inp.to(device), lab.to(device)\n",
    "        out = model(inp.long(), x_length)\n",
    "\n",
    "        predicted=torch.max(out,1)[1]\n",
    "        #print(predicted)\n",
    "        is_equal+=list((labels==predicted).data.numpy())\n",
    "\n",
    "    accuracy=sum(is_equal)/len(is_equal)\n",
    "    model.train()\n",
    "    print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "          \"Loss: {:.6f}...\".format(loss.item()),\n",
    "          \"Val accuracy: {:.6f}\".format(accuracy))\n",
    "    if accuracy <= valid_acc_max:\n",
    "        torch.save(model.state_dict(), './state_dict.pt')\n",
    "        print('Validation accuracy increased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_acc_max,accuracy))\n",
    "        valid_acc_max = accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
